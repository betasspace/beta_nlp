{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from datasets import concatenate_datasets, load_dataset, load_from_disk\n",
    "\n",
    "model_path = \"pretrained-bert\"\n",
    "vocab_size = 30_522\n",
    "max_length = 512\n",
    "\n",
    "dataset = load_from_disk(\"/home/tom/fssd/bert_dataset_longer_test\")\n",
    "d = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = d[\"train\"]\n",
    "test_dataset = d[\"test\"]\n",
    "# train_dataset = load_from_disk(\"/home/tom/fsas/bert_dataset_longer_train\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# initialize the model with the config\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens # for the Masked Language Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                mlm=True, mlm_probability=0.2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,  # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"epoch\", # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True, \n",
    "    num_train_epochs=10, # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=20, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=4, # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=64, # evaluation batch size\n",
    "    logging_steps=500, # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=10000,\n",
    "    save_total_limit=300,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "# train the model\n",
    "# whether you don't have much space so you\n",
    "# let only  3 model weights saved in the disk\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "# trainer.train(resume_from_checkpoint=f\"{model_path}/checkpoint-50000\")\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "from datasets import concatenate_datasets, load_dataset, load_from_disk\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Bert')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--epochs', default=100, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=3, type=int,\n",
    "                    metavar='N')\n",
    "parser.add_argument('-wd', '--weight_decay', default=1e-3, type=float,\n",
    "                    metavar='N')\n",
    "parser.add_argument('--local_rank', default=0, type=int, help='node rank for distributed training')\n",
    "args = parser.parse_args()\n",
    "torch.distributed.init_process_group(backend=\"nccl\") # 初始化 print(\"Use GPU: {} for training\".format(args.local_rank))\n",
    "# create model\n",
    "model_path = \"pretrained-bert\"\n",
    "vocab_size = 30_522\n",
    "max_length = 512\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "\n",
    "torch.cuda.set_device(args.local_rank) # 当前显卡 model = model.cuda() # 模型放在显卡上\n",
    "\n",
    "model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                  output_device=args.local_rank, find_unused_parameters=True) # 数据并行\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "# train_dataset = Cityscaples()\n",
    "test_dataset = load_from_disk(\"/home/tom/fsas/bert_dataset_longer_test\")\n",
    "train_sampler = DistributedSampler(train_dataset) # 分配数据\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                           shuffle=False, num_workers=args.workers,\n",
    "                                           pin_memory=True, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from datasets import concatenate_datasets, load_dataset, load_from_disk\n",
    "\n",
    "model_path = \"/home/tom/fsas/pretrained-bert\"\n",
    "# load the model checkpoint\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-200000\")) # load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "# perform predictions\n",
    "examples = [\n",
    "  \"Today's most trending hashtags on [MASK] is Donald Trump\",\n",
    "  \"The [MASK] was cloudy yesterday, but today it's rainy.\",\n",
    "]\n",
    "for example in examples:\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
